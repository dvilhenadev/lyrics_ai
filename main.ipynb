{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiricAI\n",
    "\n",
    "Aqui é mostrado um pequeno projecto em que se usa uma Recurrent Neural Network (segundo https://gilberttanner.com/blog/generating-text-using-a-recurrent-neuralnetwork) para criar a letra de uma música a partir de um determinado conjunto de letras já existente. \n",
    "\n",
    "## Os dados\n",
    "\n",
    "Os ficheiros utilizados encontram-se disponíveis no repositório de Github como ficheiros .txt\n",
    "\n",
    "## Bibliotecas a usar\n",
    "\n",
    "Como estou a seguir a ligação acima referida, Keras e Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import collections, functools, operator\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contar os caracteres presentes nas letras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '6': 17, '7': 18, '8': 19, '9': 20, ':': 21, ';': 22, '?': 23, 'a': 24, 'b': 25, 'c': 26, 'd': 27, 'e': 28, 'f': 29, 'g': 30, 'h': 31, 'i': 32, 'j': 33, 'k': 34, 'l': 35, 'm': 36, 'n': 37, 'o': 38, 'p': 39, 'q': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'x': 47, 'y': 48, 'z': 49, '{': 50, '}': 51, 'ª': 52, '«': 53, '»': 54, 'à': 55, 'á': 56, 'â': 57, 'ã': 58, 'ç': 59, 'é': 60, 'ê': 61, 'í': 62, 'ñ': 63, 'ò': 64, 'ó': 65, 'ô': 66, 'õ': 67, 'ú': 68, 'ü': 69, '\\u2005': 70, '‘': 71, '’': 72, '“': 73, '”': 74, '…': 75, '\\u205f': 76}\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./dataset\"\n",
    "files_in_dataset = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "char_indices_list = []\n",
    "indices_char_list = []\n",
    "\n",
    "total_lyrics = \"\"\n",
    "for filename in files_in_dataset:\n",
    "    filepath = dataset_path +\"/\"+filename\n",
    "    with open(filepath, 'r',encoding='utf8') as file:\n",
    "\n",
    "        text = file.read().lower()\n",
    "      \n",
    "        total_lyrics += \"\\n\\n\"+text \n",
    "\n",
    "chars = sorted(list(set(total_lyrics))) # getting all unique chars\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(total_lyrics) - max_len, step):\n",
    "    sentences.append(total_lyrics[i: i + max_len])\n",
    "    next_chars.append(total_lyrics[i + max_len])\n",
    "\n",
    "x = np.zeros((len(sentences), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(max_len, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - max_len - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + max_len]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, max_len, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "\n",
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(text) - max_len - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + max_len]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, max_len, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "number_of_files = 5\n",
    "numberchar = 250\n",
    "date = datetime.datetime.now().date()\n",
    "\n",
    "for counter in range(0, number_of_files):\n",
    "    songFileName = \"results/output_\"+str(date)+\"_\"+str(numberchar)+\"_\"+str(counter)+\".txt\"\n",
    "    with open(songFileName, 'w') as f:\n",
    "        f.write(generate_text(numberchar, 0.2))\n",
    "\n",
    "#print(generate_text(500, 0.2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
